myproject <- "CTT Office" #this is your project name on your CTT account
conn <- dbConnect(RPostgres::Postgres(), dbname=db_name)
################
outpath <- "~/Documents/radio_projects/data/radio_projects/office" #where your downloaded files are to go 1055204
d <- conn
myfiles <- list.files(file.path(outpath, myproject), recursive = TRUE, full.names = TRUE)
files_loc <- basename(myfiles)
allnode <- DBI::dbReadTable(d, "data_file")
if (fix) {
res <- DBI::dbGetQuery(d, "select distinct path from gps")
res2 <- DBI::dbGetQuery(d, "select distinct path from raw")
res1 <- DBI::dbGetQuery(d, "select distinct path from node_health")
filesdone <- c(res$path, res1$path, res2$path)
} else {
filesdone <- allnode$path
}
fix <- F
if (fix) {
res <- DBI::dbGetQuery(d, "select distinct path from gps")
res2 <- DBI::dbGetQuery(d, "select distinct path from raw")
res1 <- DBI::dbGetQuery(d, "select distinct path from node_health")
filesdone <- c(res$path, res1$path, res2$path)
} else {
filesdone <- allnode$path
}
files_import <- myfiles[which(!files_loc %in% filesdone)]
files_import <- files_import[unname(sapply(files_import, function(x) get_file_info(x)[[1]])) %in% c("gps", "node_health", "raw", "blu")]
source("~/Documents/R/celltracktech/R/api_postgres.R")
files_import <- files_import[unname(sapply(files_import, function(x) get_file_info(x)[[1]])) %in% c("gps", "node_health", "raw", "blu")]
tail(files_import)
files_import
e
# e <- file.path(outpath, myproject, e)
print(paste("attempting file import", e))
out <- get_file_info(e)
filetype <- out$filetype
sensor <- out$sensor
y <- out$y
i <- DBI::dbReadTable(conn, "ctt_project_station")
begin <- i[i$station_id == sensor, ]$deploy_at
if (length(begin) == 0) {
begin <- as.POSIXct("2018-01-01")
}
# print("attempting import")
outtest <- file_handle(e, filetype)
contents <- outtest[[1]]
errtype <- outtest[[2]]
# file_err <- fileimp[[2]]
# print("inserting contents")
#print(fix)
print(errtpe)
#print(filetype)
print(y)
errtype < 7 & errtype != 2
timecols <- c("Time") # , "recorded at", "gps at", "RecordedAt", "recorded.at", "gps.at")
for (x in timecols) {
if (x %in% names(contents)) {
contents <- dplyr::filter(contents, (!!as.name(x)) < Sys.time() & (!!as.name(x)) > begin)
}
}
contents <- data.frame(contents)
contents
head(contents)
!is.null(contents) & nrow(contents) > 0
contents$station_id <- sensor
contents$path <- y
filetype == "blu"
# print(names(contents))
contents <- contents[!is.na(contents$TagId), ]
contents$RadioId <- as.integer(contents$RadioId)
contents$TagRSSI <- as.integer(contents$TagRSSI)
contents$UsbPort <- as.integer(contents$UsbPort)
contents$BluRadioId <- as.integer(contents$BluRadioId)
contents$Sync <- as.integer(contents$Sync)
contents$Product <- as.integer(contents$Product)
contents$Revision <- as.integer(contents$Revision)
contents$Payload <- as.character(contents$Payload)
head(contents)
names(contents) <- sapply(names(contents), function(x) gsub("([[:lower:]])([[:upper:]])", "\\1_\\2", x))
names(contents) <- tolower(names(contents))
length(which(!is.na(contents$node_id))) > 0
if (length(which(!is.na(contents$node_id))) > 0) { # if there is anything beside NA nodes
nodeids <- contents$node_id[which(!is.na(contents$node_id))]
insertnew <- DBI::dbSendQuery(conn, paste("INSERT INTO ", "nodes (node_id)", " VALUES ($1)
ON CONFLICT DO NOTHING", sep = ""))
DBI::dbBind(insertnew, params = list(unique(nodeids)))
DBI::dbClearResult(insertnew)
nodecheck <- contents[!is.na(contents$node_id), ]
nodecheck <- nodecheck[!duplicated(nodecheck[c("time", "tag_id", "node_id", "tag_rssi")]), ]
badrec <- nodecheck[duplicated(nodecheck[c("time", "tag_id", "node_id")]), ]
if (nrow(badrec) > 0) {
nodecheck$id <- paste(nodecheck$time, nodecheck$tag_id, nodecheck$node_id)
badrec$id <- paste(badrec$time, badrec$tag_id, badrec$node_id)
#nodecheck <- nodecheck[!nodecheck$id %in% badrec$id, ]
nodecheck$id <- NULL
}
# print(nrow(nodecheck))
contents <- rbind(nodecheck, contents[is.na(contents$node_id), ])
if (is.na(sensor)) {
contents <- contents[is.na(contents$station_id), ] #this is for bringing in raw node files
}
}
head(contents)
length(which(nchar(contents$tag_id) != 8)) > 0
if (length(which(nchar(contents$tag_id) != 8)) > 0) { # if there are tag ids greater than 8...
contents <- contents[-which(nchar(contents$tag_id) != 8), ] # drop rows where TagId not 8 characters
}
vars <- paste(DBI::dbListFields(conn, filetype)[2:length(DBI::dbListFields(conn, filetype))], sep = "", collapse = ",")
vals <- paste(seq_along(1:(length(DBI::dbListFields(conn, filetype)) - 1)), sep = "", collapse = ", $")
contents <- contents[, DBI::dbListFields(conn, filetype)[2:length(DBI::dbListFields(conn, filetype))]]
head(contents)
any(row.names(contents) == "NA")
if (any(row.names(contents) == "NA")) {
contents <- contents[-which(row.names(contents) == "NA"), ]
}
#db_cleanup(conn)
dbDisconnect(conn)
devtools::update_packages("celltracktech")
dbDisconnect(conn)
?tempdir()
install.packages("MonetDBLite")
install.packages("duckdb")
?con
con
#conn <- dbConnect(RPostgres::Postgres(), dbname=db_name)
con <- dbConnect(duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/my-db.duckdb", read_only = FALSE)
library(DBI)
#conn <- dbConnect(RPostgres::Postgres(), dbname=db_name)
con <- dbConnect(duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/my-db.duckdb", read_only = FALSE)
#conn <- dbConnect(RPostgres::Postgres(), dbname=db_name)
con <- dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/my-db.duckdb", read_only = FALSE)
con
str(con)
is.duckdb_connection()
con@driver
format(con)
grep("duckdb", con)
grep("duckdb", format(con))
grep("kdfs", format(con))
if(grep("kdfs", format(con)))
)
length(grep("kdfs", format(con))
)
conn <- dbConnect(RPostgres::Postgres(), dbname=db_name)
conn <- dbConnect(RPostgres::Postgres(), dbname="office")
format(conn)
source("~/Documents/R/celltracktech/R/api_postgres.R")
library(celltracktech)
library(DBI)
library(RPostgres)
library(duckdb)
start <- Sys.time()
####SETTINGS#####
my_token <- "d20922a756795c9857fb1dce6c4c3bb3be4c50cd3090e30a869ab647f031cb60"
db_name <- "office"
myproject <- "CTT Office" #this is your project name on your CTT account
################
outpath <- "~/Documents/radio_projects/data/radio_projects/office" #where your downloaded files are to go 1055204
get_my_data(my_token, outpath, con, myproject=myproject)
source("~/Documents/R/celltracktech/R/api_postgres.R")
get_my_data(my_token, outpath, con, myproject=myproject)
source("~/Documents/R/celltracktech/R/api_postgres.R")
get_my_data(my_token, outpath, con, myproject=myproject)
source("~/Documents/R/celltracktech/R/api_postgres.R")
get_my_data(my_token, outpath, con, myproject=myproject)
source("~/Documents/R/celltracktech/R/api_postgres.R")
get_my_data(my_token, outpath, con, myproject=myproject)
source("~/Documents/R/celltracktech/R/api_postgres.R")
get_my_data(my_token, outpath, con, myproject=myproject)
#conn <- dbConnect(RPostgres::Postgres(), dbname=db_name)
con <- dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/my-db.duckdb", read_only = FALSE)
raw <- tbl(con, "raw")
head(raw)
devtools::update_packages("celltracktech")
#db_cleanup(conn)
dbDisconnect(con)
#conn <- dbConnect(RPostgres::Postgres(), dbname=db_name)
con <- dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/my-db.duckdb", read_only = FALSE)
library(celltracktech)
library(DBI)
library(RPostgres)
library(duckdb)
start <- Sys.time()
#conn <- dbConnect(RPostgres::Postgres(), dbname=db_name)
con <- dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/my-db.duckdb", read_only = FALSE)
tbl(con, "node_health")
y <- "/home/jess/Documents/radio_projects/data/radio_projects/office/CTT Office/V30B0154B577/node_health/CTT-V30B0154B577-node-health.2024-07-15_161339.csv.gz"
d <- con
outpath <- "/home/jess/Documents/radio_projects/data/radio_projects/office"
myproject <- "CTT Office"
conn <- con
e <- y
out <- get_file_info(e)
filetype <- out$filetype
source("~/Documents/R/celltracktech/R/api_postgres.R")
out <- get_file_info(e)
filetype <- out$filetype
filetype
sensor <- out$sensor
y <- out$y
i <- DBI::dbReadTable(conn, "ctt_project_station")
begin <- i[i$station_id == sensor, ]$deploy_at
if (length(begin) == 0) {
begin <- as.POSIXct("2018-01-01")
}
# print("attempting import")
outtest <- file_handle(e, filetype)
contents <- outtest[[1]]
errtype <- outtest[[2]]
contents
print(paste("checking file for errors:", e))
file_err <- 0
myrowfix <- c()
ignore <- FALSE
contents <- tryCatch(
{
readr::read_csv(e, col_names = TRUE)
},
error = function(err) {
return(NULL)
}
)
contents
if (filetype == "raw" & ncol(contents) > 6) {
contents <- contents[,1:6]
ignore <- TRUE
}
!is.null(contents) & nrow(contents > 0)
delete.columns <- grep("[[:digit:]]", colnames(contents), perl = T)
length(delete.columns) > 0
if (length(delete.columns) > 0) {
file_err <- 1
myrowfix <- tryCatch(
{
myrowfix <- Correct_Colnames(contents)
myrowfix[1] <- strsplit(Correct_Colnames(contents)[1], "[.]")[[1]][1]
myrowfix[2] <- strsplit(Correct_Colnames(contents)[2], "[.]")[[1]][1]
myrowfix[3] <- strsplit(Correct_Colnames(contents)[3], "\\.\\.")[[1]][1] # were there files where this wasn't correctly split?
myrowfix[4] <- strsplit(Correct_Colnames(contents)[4], "\\.\\.")[[1]][1]
myrowfix[5] <- strsplit(Correct_Colnames(contents)[5], "\\.\\.")[[1]][1]
if (nchar(myrowfix[5]) < 1) {
myrowfix[5] <- NA
}
if (length(myrowfix) > 5) {
myrowfix[6] <- strsplit(Correct_Colnames(contents)[6], "[.]")[[1]][1]
}
if (length(myrowfix) > 6) {
myrowfix[7] <- strsplit(Correct_Colnames(contents)[7], "\\.\\.")[[1]][1]
myrowfix[7] <- strsplit(myrowfix[7], "[_]")[[1]][1]
myrowfix[8] <- strsplit(Correct_Colnames(contents)[8], "\\.\\.")[[1]][1]
}
if (length(myrowfix) > 9) {
myrowfix[12] <- strsplit(Correct_Colnames(contents)[12], "\\.\\.")[[1]][1]
myrowfix[13] <- strsplit(Correct_Colnames(contents)[13], "\\.\\.")[[1]][1]
}
# rowfix <- data.frame(as.POSIXct(rowfix[1], tz="UTC"), as.integer(rowfix[2]), rowfix[3], rowfix[4], rowfix[5], as.integer(rowfix[6]))
myrowfix
# names(rowfix) <- names(contents)
# rbind(contents, rowfix)
},
error = function(err) {
return(data.frame())
}
)
# contents <- newcontents
}
ignore
if(!ignore){
rowtest <- badrow(e, contents, filetype)
contents <- rowtest[[1]]
} else {
rowtest <- list(contents,0)
#myrowfix <- c()
}
rowtest
contents
if (length(delete.columns) > 0) {
if (ncol(contents) > 9 & ncol(contents) < 14) {
names(contents) <- c("Time", "RadioId", "NodeId", "NodeRssi", "Battery", "celsius", "RecordedAt", "firmware", "SolarVolts", "SolarCurrent", "CumulativeSolarCurrent", "latitude", "longitude")
if (length(myrowfix) > 0) {
time <- timecheck(contents, myrowfix)
rowfix <- data.frame(time, as.integer(myrowfix[2]), myrowfix[3], as.integer(myrowfix[4]), as.numeric(myrowfix[5]), as.numeric(myrowfix[6]), as.POSIXct(myrowfix[7], tz = "UTC"), myrowfix[8], as.numeric(myrowfix[9]), as.numeric(myrowfix[10]), as.numeric(myrowfix[11]), as.numeric(myrowfix[12]), as.numeric(myrowfix[13]))
names(rowfix) <- names(contents)
contents <- rbind(contents, rowfix)
}
} else if (ncol(contents) < 9) {
names(contents) <- c("Time", "RadioId", "NodeId", "NodeRssi", "Battery", "celsius")
}
}
contents
devtools::update_packages("celltracktech")
devtools::update_packages("celltracktech")
con <- dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/office/my-db.duckdb", read_only = FALSE)
library(celltracktech)
library(DBI)
library(RPostgres)
library(duckdb)
con <- dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/office/my-db.duckdb", read_only = FALSE)
tbl(con, "raw") %>%
ungroup() %>%
summarise(num = n())
tbl(con, "data_file") %>%
ungroup() %>%
summarise(num = n())
dataduck <- tbl(con, "data_file") %>% pull()
dataduck
#db_cleanup(conn)
dbDisconnect(con)
source("~/Documents/R/api_run_sean.R")
source("~/Documents/R/api_run_sean.R")
tbl(con, "data_file") %>%
ungroup() %>%
summarise(num = n())
con <- dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/office/my-db.duckdb", read_only = FALSE)
tbl(con, "data_file") %>%
ungroup() %>%
summarise(num = n())
tbl(con, "raw") %>%
ungroup() %>%
summarise(num = n())
dbDisconnect(con)
###########################################################################################################################################################
##
##  Kristina L Paxton
##
##  April 1 2021
##
##    Code to examine the relationship between RSS values and distance for a given study area using an exponential decay model
##      -- Analysis in Ecology and Evolution Paper is based on data from a test with of a radio transmitter at 135 known locations distributed throughout the Guam Network
##         -- At each test location a transmitter was held stationary for a 5-minute time period
##            -- Removed first and last minute of each test to ensure that times matched between tests and the node network
##            -- For each test, calculated an average RSS value for the 3-min middle time period individually for each node that detected the transmitter
##
##
##    1st step: Data preparation - isolates raw RSS data from node network that is associated with test data
##      -- Creates the data file that was published with this paper at: https://doi.org/10.5066/P94LQWIE
##            -- Data associated with this test is coded as 'A' in the column 'DataSet'
##    2nd step: Exponential Decay Model - uses the dataset created in step 1 to examine the relationship between RSS values and distance for a node network
##
##
##    Files Needed
##        1. TestInfo.csv: file with test information that is saved in the working directory defined below
##            - For each unique test location there should a row of information associated with each of the 3 inner minutes of that test
##                - meaning that each test will have 3 rows of information with each row identifying one of the 3 inner minutes of the test (see TestInfo_Example.csv)
##            - Columns
##                -- TagId: unique identifier of the transmitter used during the specified test
##                -- TestId: unique identifier given to each of the unique locations where a test was conducted
##                -- Date: date when the specified test was conducted
##                -- Start.Time: time when the specified test was started
##                -- End.Time: time when the specified test was ended
##                -- Min: 1-minute time period of the specified test
##                -- Hour: hour of the the specified minute of the test
##                -- TestUTMx: Easting location of the specified test
##                -- TestUTMy: Northing location of the specified test
##
##       2. BeepData.rds: file with the raw RSS values collected by the node network during the time period of the test. The file should be saved in the working directory defined below
##            -- output file from Import_beep.data.Github.R script (see BeepData_Example.rds)
##            -- If using a file created from another source the following columns are needed in the specified format:
##                -- TagId: Factor identifying the unique code of a tag
##                -- Time.local: POSIXct value in the format: "2021-12-29 09:02:51" that identifies the unique datetime stamp (IN THE LOCAL TIME ZONE OF THE STUDY) when the tag was detected by the specified node
##                    ***** If you don't have a column with the local time, but only UTC time - then change lines 42-44 in Functions_RSS.Based.Localizations.R from 'Time.local' to 'Time' ************
##                    ***** BUT be sure that Dates and Times in TestInfo.csv are also in UTC time and not local time *********
##                -- NodeId: Factor identifying the node in the network that detected the specified tag
##                -- TagRSSI: Integer indicating the RSS value (in dB) of the signal of the specified tag received by the specified node
##
##       3. Nodes.csv: file with a list of all nodes in your network and their UTM locations. The file should be saved in the working directory defined below
##            - Columns
##                -- NodeId: unique identifier of given to each node
#                 -- NodeUTMx: Easting location of the specified node
##                -- NodeUTMy: Northing location of the specified node
##            - Other columns can also be in the file for your reference
##            - If Node names are being converted to scientific notation in Excel open the file with a text editor (e.g. BBedit) to change names to the correct format and save the file
##
##       4. Functions_RSS.Based.Localizations.R - R script with functions needed to run the code below - file should be saved in the working directory defined below
##
##  Important Output
##      1. Calibration Dataset that contains the average RSS values for a given node that detected the signal of a test transmitter at a known location
##      2. Model coefficients from an exponential decay model that show the relationship between RSS and Distance for a node network
##            exponential model formula: avgRSS ~ a * exp(-S * distance) + K
##                  a = intercept
##                  S = decay factor
##                  K = horizontal asymptote
##
##
##########################################################################################################################################################
# Packages needed
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
# Reset R's brain - removes all previous objects
rm(list=ls())
## Set by User
# Working Directory - Provide/path/on/your/computer/where/master/csv/file/of/nodes/is/found/and/where/Functions_CTT.Network.R/is/located
working.directory <- "/home/jess/Documents/radio_projects/EcolEvol.Manuscript_Optimizing.Trilateration"
# Directory for Output data - Provide/path/where/you/want/output/data/to/be/stored/
outpath <- "/home/jess/Documents/radio_projects/paxton/"
## Bring in functions
setwd(working.directory)
source("4_Functions_RSS.Based.Localizations.R")
## Bring in 3 Needed files - Test Information, RSS values, and Node Information - change file names in " " as needed
test.info_k <- read.csv("Test.Info_Example.csv", header = T)
test <- read.csv("~/Downloads/cal_20m_up.csv")
test$Time <- as.POSIXct(test$Time..UTC., tz="UTC")
test$lat <- format(trunc(test$Latitude*10000)/10000, nsmall=4)
test$lon <- format(trunc(test$Longitude*10000)/10000, nsmall=4)
test$id <- paste(test$lat, test$lon, sep="_")
test <- test %>%
mutate(c_diff = ifelse(id != lag(id), 1, 0))
test$c_diff[1] <- 0
test$TestId <- cumsum(test$c_diff)
library(data.table)
test.info <- setDT(test)[, .(Start.Time = min(Time), Stop.Time = max(Time)), by = TestId]
test.info$id <- test$id[match(test.info$TestId, test$TestId)]
df1 <- test %>%
group_by(TestId) %>%
summarise(TagId = list(unique(Tag.Id))) %>%
unnest(TagId)
test.info <- left_join(test.info, df1)
testid <- test[!duplicated(test$id),]
test.info$TestId <- testid$TestId[match(test.info$id, testid$id)]
test.info$lat <- as.numeric(test$lat[match(test.info$TestId, test$TestId)])
test.info$lon <- as.numeric(test$lon[match(test.info$TestId, test$TestId)])
head(test.info)
start <- min(test.info$Start.Time)
end <- max(test.info$Stop.Time)
con <- dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/office/my-db.duckdb", read_only = TRUE)
testdata <- tbl(con, "blu") |>
filter(time >= start & time <= end) |>
collect()
con <- DBI::dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/office/my-db.duckdb", read_only = TRUE)
testdata <- tbl(con, "blu") |>
filter(time >= start & time <= end) |>
collect()
dbDisconnect(con)
DBI::dbDisconnect(con)
#testdata$TestId <- 0L
colnames(test.info)[colnames(test.info)=="TagId"] <- "tag_id"
test.dat <- setDT(testdata)[test.info,  TestId := +(i.TestId), on = .(tag_id, time > Start.Time, time < Stop.Time), by = .EACHI]
test.dat <- test.dat[!is.na(test.dat$TestId),]
summary.test.tags <- test.dat %>%
dplyr::group_by(node_id, TestId) %>%
dplyr::summarise(avgRSS = mean(tag_rssi),
sdRSS = sd(tag_rssi),
n.det = n())
test.UTM <- test.info %>%
dplyr::group_by(TestId) %>%
dplyr::slice_head(n=1)
test.UTM
test.UTM$lat[1]
test.UTM$lon[1]
con <- DBI::dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/office/my-db.duckdb", read_only = TRUE)
nodes <- tbl(con, "node_health") |>
filter(time >= start & time <= end) |>
collect()
DBI::dbDisconnect(con)
nodes
start
end
end + "1 hour"
end + difftime("1 hour")
as.difftime("1 hour")
as.difftime("1 hours")
difftime(1, units="hours")
nodes <- tbl(con, "node_health") |>
filter(time >= start - 60*60 & time <= end + 60*60) |>
collect()
con <- DBI::dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/office/my-db.duckdb", read_only = TRUE)
nodes <- tbl(con, "node_health") |>
filter(time >= start - 60*60 & time <= end + 60*60) |>
collect()
nodes
start - 60*60
start_buff = start - 60*60
end_buff = end + 60*60
nodes <- tbl(con, "node_health") |>
filter(time >= start_buff  & time <= end_buff) |>
collect()
nodes
start_buff = start - 2*60*60
end_buff = end + 2*60*60
nodes <- tbl(con, "node_health") |>
filter(time >= start_buff  & time <= end_buff) |>
collect()
nodes
start_buff
end_buff
nodes <- tbl(con, "node_health") |>
#filter(time >= start_buff  & time <= end_buff) |>
collect()
nodes
DBI::dbDisconnect(con)
con <- DBI::dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/office/my-db.duckdb", read_only = TRUE)
nodes <- tbl(con, "node_health") |>
#filter(time >= start_buff  & time <= end_buff) |>
collect()
nodes
DBI::dbDisconnect(con)
con <- DBI::dbConnect(duckdb::duckdb(), dbdir = "/home/jess/Documents/radio_projects/data/radio_projects/office/my-db.duckdb", read_only = TRUE)
nodes <- tbl(con, "data_files") |>
#filter(time >= start_buff  & time <= end_buff) |>
collect()
nodes <- tbl(con, "data_file") |>
#filter(time >= start_buff  & time <= end_buff) |>
collect()
nodes
grep("node", nodes$path)
grep("raw", nodes$path)
source("~/Documents/R/api_run_sean.R")
#db_cleanup(conn)
dbDisconnect(con)
source("~/Documents/R/api_run_sean.R")
